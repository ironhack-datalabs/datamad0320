{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1: PySpark Demo\n",
    "\n",
    "In this Challenge we'll show you how to use Spark to run intensive computation jobs in Jupyter Notebook. \n",
    "\n",
    "So far you should have installed PySpark in local mode, configured your environment variables, and started Jupyter Notebook with PySpark. If you haven't completed any of the previous steps, please go back and complete them because otherwise you won't be able to complete this challenge.\n",
    "\n",
    "If everything works so far, you should be able to create a `SparkContext` instance in Jupyter Notebook. Try to execute the next cell where the code is already written for you to create a new `SparkContext` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a `SparkContext` instance or reuse an existing one\n",
    "from pyspark import SparkContext \n",
    "sc = SparkContext.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No errors? Congrats! But in case you do, review the lesson and google your error to see what could have possibly gone wrong.\n",
    "\n",
    "SparkContext is a library that helps you access the Spark execution environment in order to use the [Spark Python APIs](https://spark.apache.org/docs/latest/api/python/index.html). We will show you an example of how to call the Spark API using the SparkContext instance.\n",
    "\n",
    "But before we start the real work, we'd like you to get familiar with the concept of [benchmarking](https://en.wikipedia.org/wiki/Benchmark_(computing)). Benchmarking is a common technique in software engineering and data engineering to evaluate the efficiency of your codes. The idea is to measure the execution time of your program and research if there is room to improve your code in order to get the job done with shorter time. In machine learning a lot of times you'be doing repeated sampling and complex queries that could run for hours, days, or even weeks. It is important that you improve your code efficiency whenever you can. Otherwise you'll waste a lot of time and computation powers in executing inefficient code.\n",
    "\n",
    "We will benchmark a code snippet that estimates Pi (`π`) by repeated sampling. We will first benchmark this without using Spark. The idea of the Pi estimation is to randomly generate many points with `x` and `y` coordinates between 0 and 1. These points will fall in a square whose side length is 1 (the upper right grid of the square in the image below that combines the red and green areas). We will count the number of sample points that fall into the 1/4 circle sector (the area in red) against all points which allows us to calculate Pi.\n",
    "\n",
    "![pi.png](pi.png)\n",
    "\n",
    "Below are the math formulas to calculate the probability of a point to fall into the 1/4 circle sector (*A0*) and the square area (*A0*).\n",
    "\n",
    "```\n",
    "A0 = π * r^2\n",
    "A1 = (2r)^2\n",
    "```\n",
    "\n",
    "From the formulas above, you can deduce ` π = 4 * A0` if *r* is 1, which means Pi equals to 4 times the probability of a point falling into the red area.\n",
    "\n",
    "Because this lab focuses on Spark, we will provide you the code snippet for estimating Pi based on the mathematical concept discussed above. Read the code carefully and make sure you fully understand what the code does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def inside(p):\n",
    "    \"\"\"\n",
    "    Generate a random point and check if the point is within the circle with radius=1.\n",
    "    \n",
    "    Returns:\n",
    "        (bool) whether the generated point is within the circle area\n",
    "    \"\"\"\n",
    "    x, y = np.random.random(), np.random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "def estimate_pi(num_samples):\n",
    "    \"\"\"\n",
    "    Estimate the value of Pi by means of repeated sampling. Benchmark the repeated sampling time cost.\n",
    "    \n",
    "    Params:\n",
    "        num_sampes (int): the number of sample points to generate\n",
    "    \n",
    "    Returns:\n",
    "        (float) estimated value of Pi\n",
    "    \"\"\"\n",
    "    print(\"Executing Spark job...\")\n",
    "    start = timer()\n",
    "    dots = list(filter(inside,list(range(num_samples)))) \n",
    "    count = len(dots)\n",
    "    end = timer()\n",
    "    print(\"Spark job ended. Total time elapsed: %s\" % (end-start))\n",
    "    return(4.0 * count / num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, call `estimate_pi()` with `50000`, `500000`, and `5000000` sample sizes. See what you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Spark job...\n",
      "Spark job ended. Total time elapsed: 0.06604767900716979\n",
      "Executing Spark job...\n",
      "Spark job ended. Total time elapsed: 0.5845926219917601\n",
      "Executing Spark job...\n",
      "Spark job ended. Total time elapsed: 5.922161819995381\n",
      "[3.1476, 3.142592, 3.1409232]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "samples=[50000, 500000, 5000000]\n",
    "res=[estimate_pi(ele) for ele in samples]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the execution time increases when you increase the sample size. In addition, the accuracy of the Pi value increases as you increase the sample size. But if you keep increasing the sample size you have to wait a long time for the script to finish execution. What can you do if you really need to run a very large sample size?\n",
    "\n",
    "There are several options. You can **use a computer with higher processing speed**. This allows you to execute the sampling jobs faster. But this is usually not the best option because the improvement of CPU processing speed has limited impacts on executing millions of sampling jobs.\n",
    "\n",
    "The second option is to **run multiple sampling jobs in parallel**. In case you don't know yet, Python is a linear programming language which means it executes one job then the next. If you wait for millions of samping jobs to finish in a single queue it will take you a lot of time. Therefore, most modern programming languages including Python and R have introduced a feature called [parallel programming or parallel processing](https://wiki.python.org/moin/ParallelProcessing) so that you can save time by running several tasks at the same time. Say if your sampling jobs would take 1,000 seconds to finish in a single queue, you can finish them in 250 seconds with 4 parallel processing queues. Note that this time calculation is theoretical. In reality with 4 parallel queues the finishing time is usually more than 250 seconds because Python has to spend some overhead time to [spawn](https://en.wikipedia.org/wiki/Spawn_(computing)) the parallel processes.\n",
    "\n",
    "**How many parallel processes you can run depends on how many CPU cores you have** on your computer. If you have 8 CPU cores on your computer, the max number of parallel processes you can run is 8. What if you attempt to run more processes than the number of your CPU cores? The excessive jobs will be queued up in the same CPU core which makes your parallel programming meaningless. Therefore, in order to run parallel programming you need to use a multi-core computer and the parallel processes can't exceed your number of CPU cores. \n",
    "\n",
    "Then you may ask, **should I allocate all my CPU cores to parallel programming?** Remember that besides executing the parallel programming scripts, your computer's CPUs also need to run your operation system and some other system processes. Therefore, **you typically reserve 1-2 CPU cores for the OS and system processes and allocate the rest to parallel programming**. However, with Apache Spark you don't need to manually allocate the CPU cores because Spark will automatically do that for you depending on how many cores it has access to. Remember in the lesson you used the following command to start Spark:\n",
    "\n",
    "```\n",
    "$SPARK_PATH/bin/pyspark --master \"local[*]\"\n",
    "```\n",
    "\n",
    "The `*` directive tells Sparks to decide how many CPU cores to use by itself. If you start Spark with `local[2]`, it means you allocate 2 CPU cores to Spark.\n",
    "\n",
    "Another note is **your computer memory may also set a bottleneck on the performance of parallel programming**. When your computer runs parallel jobs, its memory consumption is multiplied. If a single process consumes 256mb memory, four parallel processes will consume 1,024mb memory. This is in addition to the memory consumption of Apache Spark that keeps your dataset in the memory to improve processing speed. Therefore, in order to use Spark successfully in local mode, you really need a high-end computer with multi-cores and high memory (8gb or above). If your computer runs out of memory, your parallel scripts will be shut down.  \n",
    "\n",
    "So far we have discussed the multi-core and memory concerns of running Spark in the local mode. These concerns are similar if you run Spark in the cluster mode. With the cluster mode each cluster has its own memory and one or multiple cores. Spark is smart enough to automatically allocate the resources in the cluster mode too.\n",
    "\n",
    "Now let's move on to the real stuff. How do you run parallel programming with Apache Spark? **That is achieved by invoking Spark's `parallelize` method** ([documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html#parallelized-collections)). In the next cell, we have a function called `estimate_pi_parallel` for you to complete. You will rewrite the `estimate_pi` function we gave you earlier and use `SparkContext.parallelize` to spawn parallel processes to create the random point samples then use the samples to estimate Pi. \n",
    "\n",
    "If you are stuck, you can reference [this Jupyter Notebook example](https://github.com/mGalarnyk/Installations_Mac_Ubuntu_Windows/blob/master/Spark/Estimating%20PI.ipynb) (but don't simply copy and paste). **Make sure to benchmark your function so that we know if Spark helps improve the code executing time.**\n",
    "\n",
    "*Hint: You can re-use the `inside` function we gave you earlier in your code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_pi_parallel(num_samples):\n",
    "    \"\"\"\n",
    "    Estimate the value of Pi by means of repeated sampling using Spark's `parallelize` method. \n",
    "    Benchmark the repeated sampling time cost.\n",
    "    \n",
    "    Params:\n",
    "        num_sampes (int): the number of sample points to generate\n",
    "    \n",
    "    Returns:\n",
    "        (float) estimated value of Pi\n",
    "    \"\"\"\n",
    "    print(\"Executing Spark job...\")\n",
    "    start = timer()\n",
    "    rdd = sc.parallelize(range(0, num_samples)).filter(inside)\n",
    "    print(rdd)\n",
    "    cuenta= rdd.count()\n",
    "    end = timer()\n",
    "    print(\"Spark job ended. Total time elapsed: %s\" % (end-start))\n",
    "    return(4 * cuenta / num_samples)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, test your `estimate_pi_parallel` function with `5000000` and `50000000` sample sizes. Run it more than once for each sample size to see how the execution time varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Spark job...\n",
      "PythonRDD[1] at RDD at PythonRDD.scala:53\n",
      "Spark job ended. Total time elapsed: 2.432726511004148\n",
      "Executing Spark job...\n",
      "PythonRDD[4] at RDD at PythonRDD.scala:53\n",
      "Spark job ended. Total time elapsed: 0.802028516001883\n",
      "Executing Spark job...\n",
      "PythonRDD[7] at RDD at PythonRDD.scala:53\n",
      "Spark job ended. Total time elapsed: 6.034824030008167\n",
      "Executing Spark job...\n",
      "PythonRDD[10] at RDD at PythonRDD.scala:53\n",
      "Spark job ended. Total time elapsed: 59.489230164996115\n",
      "[3.1536, 3.15008, 3.1418624, 3.1413552]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "samples=[50000, 500000, 5000000,50000000]\n",
    "res=[estimate_pi_parallel(ele) for ele in samples]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have noticed the execution time is significantly shorter with Spark's `parallelize` method than without using Spark. However, if you find the execution time is actually longer, it could be because your code is not efficient or your computer doesn't have multiple CPU cores to offer to Spark. If it's the latter case, don't worry. You can still experience the power of parallel programming in Challenge 2 when we run Spark in the cluster mode.\n",
    "\n",
    "You may have also noticed the first time to execute your function takes significantly longer time than executing it in the second and third time (you need to execute the commands consequtively in a row). This is because at the first time you execute the function, Spark has to acquire the memory and CPUs it needs from the system. After the execution is finished Spark does not release the memory immediately. So when you execute the script consequtively, Spark does not need to acquire the hardware resources again.\n",
    "\n",
    "Spark is written in Java (you are interacting with its Java core via a Python wrapper). Java applications typically maintain a minimum memory even if it is idling. When memory-demanding processes are being executed, Java will *burst* its memory consumption by acquiring more from the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling with PySpark Dataframes\n",
    "\n",
    "Next, you will be practicing data wrangling with the Pandas-like dataframes that PySpark provides you. You should have already used the equivalent of these functions with Pandas in the previous labs. We just want to show you the Spark way of doing the similar things. Note that the syntax in Pyspark is different from that in Pandas. Use Google to find the examples you need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import `SparkSession` from `pyspark.sql`. Create a new instance of SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('patients').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read a CSV dataset from a previous lab using `spark.read.csv()` and assign the returned dataframe into a variable called `patients`.\n",
    "\n",
    "The dataset path is `../../lab-sklearn-and-unsupervised-learning/patient-admission-dataset-for-learning-data-mining.csv`. \n",
    "\n",
    "*Hint: Use `spark.read.csv()` to read from the dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `spark.read.csv`\n",
    "patients=spark.read.csv('../../lab-sklearn-and-unsupervised-learning/patient-admission-dataset-for-learning-data-mining.csv',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print a summary of `patients` using `printSchema()`.\n",
    "\n",
    "*Hint: Use `printSchema`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- patient_name: string (nullable = true)\n",
      " |-- patient_email: string (nullable = true)\n",
      " |-- doctor_phone: string (nullable = true)\n",
      " |-- patient_gender: string (nullable = true)\n",
      " |-- patient_dob: string (nullable = true)\n",
      " |-- patient_diabetic: string (nullable = true)\n",
      " |-- patient_allergic: string (nullable = true)\n",
      " |-- patient_weight_kg: string (nullable = true)\n",
      " |-- patient_height_sm: string (nullable = true)\n",
      " |-- patient_nhs_number: string (nullable = true)\n",
      " |-- doctor_name: string (nullable = true)\n",
      " |-- appointment_date: string (nullable = true)\n",
      " |-- patient_show: string (nullable = true)\n",
      " |-- is_regular_visit: string (nullable = true)\n",
      " |-- prescribed_medicines: string (nullable = true)\n",
      " |-- diagnosis: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "patients.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the missing values in the `patients[\"diagnosis\"]`. Your output should be `488`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "488"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "from pyspark.sql.functions import isnan, when, count\n",
    "\n",
    "patients.select('diagnosis').withColumn('isNull_diag',patients['diagnosis'].isNull()).where('isNull_diag = True').count()\n",
    "#len(patients.groupBy('diagnosis').count().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the `diagnosis` column in `patients`.\n",
    "\n",
    "*Hint: First select the column (`.select()`) then use the `show()` method*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|diagnosis|\n",
      "+---------+\n",
      "|     I669|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|  S72309N|\n",
      "|     null|\n",
      "|   T508X6|\n",
      "|     null|\n",
      "|     S420|\n",
      "|    T8743|\n",
      "|  M80072A|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|  T22342A|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "patients.select('diagnosis').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill the missing values in column `diagnosis` with a string `no diagnosis`.\n",
    "\n",
    "*Hint: Use `.fillna()`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "patients=patients.na.fill({'diagnosis':'no diagnosis'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the `diagnosis` column again to confirm null values are replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   diagnosis|\n",
      "+------------+\n",
      "|        I669|\n",
      "|no diagnosis|\n",
      "|no diagnosis|\n",
      "|no diagnosis|\n",
      "|no diagnosis|\n",
      "|no diagnosis|\n",
      "|     S72309N|\n",
      "|no diagnosis|\n",
      "|      T508X6|\n",
      "|no diagnosis|\n",
      "|        S420|\n",
      "|       T8743|\n",
      "|     M80072A|\n",
      "|no diagnosis|\n",
      "|no diagnosis|\n",
      "|no diagnosis|\n",
      "|     T22342A|\n",
      "|no diagnosis|\n",
      "|no diagnosis|\n",
      "|no diagnosis|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "patients.select('diagnosis').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the missing values in column `doctor_name`. You should see output `58`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "patients.select('doctor_name').withColumn('doctor_name',patients['doctor_name'].isNull()).where('doctor_name = True').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop the rows in `patients` where column `doctor_name` is missing data. Remember to assign the converted dataset back to `patients`.\n",
    "\n",
    "*Hint: Use `.dropna()`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "patients=patients.na.fill({'diagnosis':'no diagnosis'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now count the missing values in `doctor_name` again. You should see output `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "patients=patients.dropna(subset='doctor_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the boolean columns `patient_show`, `is_regular_visist`, `patient_diabetic`, and `patient_allergic` to int.\n",
    "\n",
    "*Hint: [Here](https://stackoverflow.com/questions/33354571/casting-a-new-derived-column-in-a-dataframe-from-boolean-to-integer) is an example.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "colu=['patient_show', 'is_regular_visit','patient_diabetic', 'patient_allergic']\n",
    "\n",
    "for ele in colu:\n",
    "    patients=patients.withColumn(ele, when(patients[ele]==False,0).otherwise(1).cast('integer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a new column called `diagnosis_int`. The value in this column should be `0` if the corresponding row of the `diagnosis` column is `no diagnosis`. Otherwise the value should be `1`.\n",
    "\n",
    "*Hint: The way to add a derived coloumn is similar to converting a column values as you've done in the previous question.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "patients=patients.withColumn('diagnosis_int', when(patients.diagnosis=='no diagnosis',0).otherwise(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a new column called `gender_int`. The value in this column should be `0` if the corresponding row of the `patient_gender` column is `Male`. Otherwise the value should be `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "patients=patients.withColumn('gender_int', when(patients.patient_gender=='Male',0).otherwise(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the time limitation for this lab, we won't go through all steps of data wrangling like we did in the Unsupervised Learning with Scikit-Learn Lab. We'll simply keep the numerical columns we have so far and drop the rest so that we'll have time to practice MLlib.\n",
    "\n",
    "#### Drop the following columns from `patients`: \n",
    "\n",
    "```\n",
    "['id', 'patient_name', 'patient_email', 'patient_nhs_number', 'doctor_phone', 'patient_dob', 'doctor_name', 'appointment_date', 'prescribed_medicines', 'diagnosis', 'patient_gender']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "to_drop=['id', 'patient_name', 'patient_email', 'patient_nhs_number', 'doctor_phone', 'patient_dob', 'doctor_name', 'appointment_date', 'prescribed_medicines', 'diagnosis', 'patient_gender']\n",
    "\n",
    "patients=patients.drop(*to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call `printSchema()` again for `patients`. You should see all fields remaining are interger types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- patient_diabetic: integer (nullable = false)\n",
      " |-- patient_allergic: integer (nullable = false)\n",
      " |-- patient_weight_kg: string (nullable = true)\n",
      " |-- patient_height_sm: string (nullable = true)\n",
      " |-- patient_show: integer (nullable = false)\n",
      " |-- is_regular_visit: integer (nullable = false)\n",
      " |-- diagnosis_int: integer (nullable = false)\n",
      " |-- gender_int: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "patients.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "colu=['patient_weight_kg', 'patient_height_sm']\n",
    "\n",
    "for ele in colu:\n",
    "    patients=patients.withColumn(ele, patients[ele].cast('float'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You now get the feeling that PySpark have similar features as Pandas to perform data wrangling. However data wrangling with PySpark is not as convenient as in Pandas because PySpark is specialized for data engineering rather than data processing. The real value of Spark is to compute really big datasets with complex and time-consuming algorithms. Many data scientists/engieers use Pandas for data wrangling then import the data to Spark for computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans Analysis Example\n",
    "\n",
    "Finally we will show you an example of clustering the cleaned data with KMeans. Because you have done the same thing in the Supervised Learning with Scikit-Learn lab, we won't ask you to figure it out with PySpark's MLlib. We are providing the code for you to reference.\n",
    "\n",
    "In order to perform KMeans analysis in PySpark, **we need to first use the `VectorAssembler` to convert the existing data into high-dimensional vectors**. We will assign the generated vectors into a new column called `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|features                             |\n",
      "+-------------------------------------+\n",
      "|[0.0,1.0,59.0,176.0,1.0,1.0,1.0,1.0] |\n",
      "|[0.0,1.0,77.0,186.0,1.0,1.0,0.0,1.0] |\n",
      "|[1.0,1.0,90.0,177.0,0.0,0.0,0.0,1.0] |\n",
      "|[1.0,1.0,70.0,150.0,0.0,1.0,0.0,0.0] |\n",
      "|(8,[1,2,3,7],[1.0,82.0,140.0,1.0])   |\n",
      "|[0.0,1.0,105.0,179.0,0.0,1.0,0.0,1.0]|\n",
      "|[1.0,1.0,87.0,180.0,0.0,0.0,1.0,0.0] |\n",
      "|(8,[2,3,7],[73.0,152.0,1.0])         |\n",
      "|(8,[0,2,3,6],[1.0,83.0,156.0,1.0])   |\n",
      "|(8,[2,3,4],[89.0,160.0,1.0])         |\n",
      "|[1.0,1.0,108.0,165.0,0.0,1.0,1.0,0.0]|\n",
      "|[0.0,0.0,53.0,147.0,0.0,1.0,1.0,1.0] |\n",
      "|[1.0,0.0,51.0,170.0,1.0,1.0,1.0,0.0] |\n",
      "|[1.0,1.0,106.0,142.0,1.0,0.0,0.0,0.0]|\n",
      "|(8,[1,2,3,7],[1.0,58.0,169.0,1.0])   |\n",
      "|(8,[0,2,3],[1.0,64.0,150.0])         |\n",
      "|[1.0,0.0,56.0,168.0,1.0,1.0,1.0,1.0] |\n",
      "|[0.0,1.0,69.0,182.0,1.0,0.0,0.0,1.0] |\n",
      "|(8,[1,2,3,5],[1.0,69.0,178.0,1.0])   |\n",
      "|[0.0,1.0,77.0,155.0,1.0,1.0,0.0,1.0] |\n",
      "+-------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"patient_diabetic\", \"patient_allergic\", \"patient_weight_kg\", \"patient_height_sm\", \"patient_show\", \"is_regular_visit\", \"diagnosis_int\", \"gender_int\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(patients)\n",
    "output.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we import the `KMeans` class from Pyspark's MLlib and fit the new dataset with the `features` column to KMeans. We call `KMeans().setK(4)` to indicate we would like to receive 4 data clusters. Finally we print the center of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Errors = 119574.87827302358\n",
      "Cluster Centers: \n",
      "[  0.51260504   0.52941176  62.76890756 176.96638655   0.47058824\n",
      "   0.5          0.44957983   0.55042017]\n",
      "[  0.55504587   0.51834862  97.75229358 153.49541284   0.49082569\n",
      "   0.50458716   0.59174312   0.48165138]\n",
      "[  0.50643777   0.49785408  65.5751073  152.26180258   0.50643777\n",
      "   0.55364807   0.50643777   0.44635193]\n",
      "[  0.48616601   0.49407115  93.88142292 178.70355731   0.50988142\n",
      "   0.52964427   0.51778656   0.53754941]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans().setK(4).setSeed(1)\n",
    "\n",
    "model = kmeans.fit(output)\n",
    "\n",
    "wssse = model.computeCost(output)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
